learner = setHyperPars(learner, par.vals = best.parvals)
return(regressionCombined(learner = learner, data = data, measure = scenarioLeaveOne$desc$performance_measures,
metrics = metricsLeaveOne))
})
rest.model$models = lapply(outer.preds, function(x) { x$models } )
rest.model$predictions = rbindlist(lapply(outer.preds, function(x) { x$predictions }))
#set up data for left out solver
scenarioLeftSolver = scenario
#leave one solver in description
scenarioLeftSolver$desc$algorithms_deterministic = subset(scenarioLeftSolver$desc$algorithms_deterministic, scenarioLeftSolver$desc$algorithms_deterministic == solver)
scenarioLeftSolver$desc$algorithms_stochastic = subset(scenarioLeftSolver$desc$algorithms_stochastic, scenarioLeftSolver$desc$algorithms_stochastic == solver)
scenarioLeftSolver$desc$metainfo_algorithms = scenarioLeftSolver$desc$metainfo_algorithms[solver]
#leave one solver in runs and status
scenarioLeftSolver$algo.runs = subset(scenarioLeftSolver$algo.runs, scenarioLeftSolver$algo.runs$algorithm == solver)
scenarioLeftSolver$algo.runstatus = select(scenarioLeftSolver$algo.runstatus, c("instance_id", "repetition", solver))
#leave on solver from metrics
metricsLeftSolver = subset(metrics, metrics$algorithm == solver)
llama.cv.solver = convertToLlamaCVFolds(scenarioLeftSolver)
data.solver = llama.cv.solver
data.solver$train = list(llama.cv.solver$train[[fold.n]])
data.solver$test = list(llama.cv.solver$test[[fold.n]])
solverLeft.preds = regressionLeftSolver(model = rest.model$models, data = data.solver, train.data = llama.cv, train.metrics = metricsLeaveOne,
new.metrics = metricsLeftSolver, measure = scenarioLeftSolver$desc$performance_measures)
solver.preds = list(solverLeft = solverLeft.preds, rest.model = rest.model)
return(solver.preds)
}
#train
outer.preds = trainModel(llama.cv, learner, inputData$metrics, n.inner.folds, timeout, rs.iters, par.set, inputData$scenario, fold.n, solver.id)
#train model with nested cross-validation and tuning
trainModel = function(llama.cv, learner, metrics, n.inner.folds, timeout, rs.iters, par.set, scenario, fold.n, solver.id) {
#train on each train split
rest.model = vector("list")
solver = as.character(metrics$algorithm[[solver.id]])
set.seed(1, "L'Ecuyer")
scenarioLeaveOne = scenario
#remove solver from description
scenarioLeaveOne$desc$algorithms_deterministic = subset(scenarioLeaveOne$desc$algorithms_deterministic, scenarioLeaveOne$desc$algorithms_deterministic != solver)
scenarioLeaveOne$desc$algorithms_stochastic = subset(scenarioLeaveOne$desc$algorithms_stochastic, scenarioLeaveOne$desc$algorithms_stochastic != solver)
scenarioLeaveOne$desc$metainfo_algorithms[[solver]] = NULL
#remove solver from runs and status
scenarioLeaveOne$algo.runs = subset(scenarioLeaveOne$algo.runs, scenarioLeaveOne$algo.runs$algorithm != solver)
scenarioLeaveOne$algo.runstatus[[solver]] = NULL
#remove solver from metrics
metricsLeaveOne = subset(metrics, metrics$algorithm != solver)
llama.cv = convertToLlamaCVFolds(scenarioLeaveOne)
outer.preds = lapply(fold.n:fold.n, function(k) {
#data for inner cv split
inner.data = llama.cv
inner.data$data = llama.cv$data[llama.cv$train[[k]],]
inner.data$train = NULL
inner.data$test = NULL
inner.newdata = cvFolds(inner.data, nfolds = n.inner.folds, stratify = FALSE)
data = inner.newdata
#obtain best hyperparameters
best.parvals = tuneModel(data, learner, rs.iters, par.set, scenarioLeaveOne, metricsLeaveOne)
#make test and train data split
data = llama.cv
data$train = list(llama.cv$train[[k]])
data$test = list(llama.cv$test[[k]])
learner = setHyperPars(learner, par.vals = best.parvals)
return(regressionCombined(learner = learner, data = data, measure = scenarioLeaveOne$desc$performance_measures,
metrics = metricsLeaveOne))
})
rest.model$models = lapply(outer.preds, function(x) { x$models } )
rest.model$predictions = rbindlist(lapply(outer.preds, function(x) { x$predictions }))
#set up data for left out solver
scenarioLeftSolver = scenario
#leave one solver in description
scenarioLeftSolver$desc$algorithms_deterministic = subset(scenarioLeftSolver$desc$algorithms_deterministic, scenarioLeftSolver$desc$algorithms_deterministic == solver)
scenarioLeftSolver$desc$algorithms_stochastic = subset(scenarioLeftSolver$desc$algorithms_stochastic, scenarioLeftSolver$desc$algorithms_stochastic == solver)
scenarioLeftSolver$desc$metainfo_algorithms = scenarioLeftSolver$desc$metainfo_algorithms[solver]
#leave one solver in runs and status
scenarioLeftSolver$algo.runs = subset(scenarioLeftSolver$algo.runs, scenarioLeftSolver$algo.runs$algorithm == solver)
scenarioLeftSolver$algo.runstatus = select(scenarioLeftSolver$algo.runstatus, c("instance_id", "repetition", solver))
#leave on solver from metrics
metricsLeftSolver = subset(metrics, metrics$algorithm == solver)
llama.cv.solver = convertToLlamaCVFolds(scenarioLeftSolver)
data.solver = llama.cv.solver
data.solver$train = list(llama.cv.solver$train[[fold.n]])
data.solver$test = list(llama.cv.solver$test[[fold.n]])
solverLeft.preds = regressionLeftSolver(model = rest.model$models, data = data.solver, train.data = llama.cv, train.metrics = metricsLeaveOne,
new.metrics = metricsLeftSolver, measure = scenarioLeftSolver$desc$performance_measures)
solver.preds = list(solverLeft = solverLeft.preds, rest.model = rest.model)
return(solver.preds)
}
#train
outer.preds = trainModel(llama.cv, learner, inputData$metrics, n.inner.folds, timeout, rs.iters, par.set, inputData$scenario, fold.n, solver.id)
solver.id
#train model with nested cross-validation and tuning
trainModel = function(llama.cv, learner, metrics, n.inner.folds, timeout, rs.iters, par.set, scenario, fold.n, solver.n) {
#train on each train split
rest.model = vector("list")
solver = as.character(metrics$algorithm[[solver.n]])
set.seed(1, "L'Ecuyer")
scenarioLeaveOne = scenario
#remove solver from description
scenarioLeaveOne$desc$algorithms_deterministic = subset(scenarioLeaveOne$desc$algorithms_deterministic, scenarioLeaveOne$desc$algorithms_deterministic != solver)
scenarioLeaveOne$desc$algorithms_stochastic = subset(scenarioLeaveOne$desc$algorithms_stochastic, scenarioLeaveOne$desc$algorithms_stochastic != solver)
scenarioLeaveOne$desc$metainfo_algorithms[[solver]] = NULL
#remove solver from runs and status
scenarioLeaveOne$algo.runs = subset(scenarioLeaveOne$algo.runs, scenarioLeaveOne$algo.runs$algorithm != solver)
scenarioLeaveOne$algo.runstatus[[solver]] = NULL
#remove solver from metrics
metricsLeaveOne = subset(metrics, metrics$algorithm != solver)
llama.cv = convertToLlamaCVFolds(scenarioLeaveOne)
outer.preds = lapply(fold.n:fold.n, function(k) {
#data for inner cv split
inner.data = llama.cv
inner.data$data = llama.cv$data[llama.cv$train[[k]],]
inner.data$train = NULL
inner.data$test = NULL
inner.newdata = cvFolds(inner.data, nfolds = n.inner.folds, stratify = FALSE)
data = inner.newdata
#obtain best hyperparameters
best.parvals = tuneModel(data, learner, rs.iters, par.set, scenarioLeaveOne, metricsLeaveOne)
#make test and train data split
data = llama.cv
data$train = list(llama.cv$train[[k]])
data$test = list(llama.cv$test[[k]])
learner = setHyperPars(learner, par.vals = best.parvals)
return(regressionCombined(learner = learner, data = data, measure = scenarioLeaveOne$desc$performance_measures,
metrics = metricsLeaveOne))
})
rest.model$models = lapply(outer.preds, function(x) { x$models } )
rest.model$predictions = rbindlist(lapply(outer.preds, function(x) { x$predictions }))
#set up data for left out solver
scenarioLeftSolver = scenario
#leave one solver in description
scenarioLeftSolver$desc$algorithms_deterministic = subset(scenarioLeftSolver$desc$algorithms_deterministic, scenarioLeftSolver$desc$algorithms_deterministic == solver)
scenarioLeftSolver$desc$algorithms_stochastic = subset(scenarioLeftSolver$desc$algorithms_stochastic, scenarioLeftSolver$desc$algorithms_stochastic == solver)
scenarioLeftSolver$desc$metainfo_algorithms = scenarioLeftSolver$desc$metainfo_algorithms[solver]
#leave one solver in runs and status
scenarioLeftSolver$algo.runs = subset(scenarioLeftSolver$algo.runs, scenarioLeftSolver$algo.runs$algorithm == solver)
scenarioLeftSolver$algo.runstatus = select(scenarioLeftSolver$algo.runstatus, c("instance_id", "repetition", solver))
#leave on solver from metrics
metricsLeftSolver = subset(metrics, metrics$algorithm == solver)
llama.cv.solver = convertToLlamaCVFolds(scenarioLeftSolver)
data.solver = llama.cv.solver
data.solver$train = list(llama.cv.solver$train[[fold.n]])
data.solver$test = list(llama.cv.solver$test[[fold.n]])
solverLeft.preds = regressionLeftSolver(model = rest.model$models, data = data.solver, train.data = llama.cv, train.metrics = metricsLeaveOne,
new.metrics = metricsLeftSolver, measure = scenarioLeftSolver$desc$performance_measures)
solver.preds = list(solverLeft = solverLeft.preds, rest.model = rest.model)
return(solver.preds)
}
#train
outer.preds = trainModel(llama.cv, learner, inputData$metrics, n.inner.folds, timeout, rs.iters, par.set, inputData$scenario, fold.n, solver.n)
outer.preds
combined_model = outer.preds$solverLeft
combined_model = outer.preds$solverLeft
test.preds = outer.preds$solverLeft$test.predictions
test.preds
test.model = outer.preds$solverLeft
test.model$train.predictions = NULL
test.model$test.predictions = NULL
test.model$predictions = test.preds
combined_model
combined_model$test.predictions
# sort predictions
if(inputData$scenario$desc$maximize[[inputData$scenario$desc$performance_measures]]) {
test.model$predictions = test.model$predictions[order(test.model$predictions$instance_id, -test.model$predictions$score)]
} else {
test.model$predictions = test.model$predictions[order(test.model$predictions$instance_id, test.model$predictions$score)]
}
combined_model = list(test.model = test.model)
rest.model = outer.preds$rest.model$models
names = inputData$metrics$algorithm[[solver.n]]
rest.model = setNames(rest.model, names)
rest.model$predictions = outer.preds$rest.model$predictions
rest.model$predictions
combined_model$test.model$predictions
combined_model$test.model
getwd()
opt$save = "./manual-folds-manual-solvers/"
save(combined_model, file = sprintf("%s/proposed_leave_one_out_%s_%s_%s.RData", opt$save, inputData$scenario$desc$scenario_id, solver.n, fold.n))
save(rest.model, file = sprintf("%s/proposed_leave_one_out_rest_%s_%s_%s.RData", opt$save, inputData$scenario$desc$scenario_id, solver.n, fold.n))
combined_model$test.model$predictions
unqiue(combined_model$test.model$predictions$algorithm)
unique(combined_model$test.model$predictions$algorithm)
rest.model$predictions
library(shiny); runApp('learn-to-shine/compareSelectors.R')
library(shiny)
library(mlr)
library(llama)
library(aslib)
runApp('learn-to-shine/compareSelectors.R')
library(shiny)
library(mlr)
library(llama)
library(aslib)
library(ggplot2)
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
library(plotly)
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
library(shiny); runApp('learn-to-shine/compare-selectors/compareSelectors.R')
?deparse
?substitute
x = "mpg"
data = mtcars
head(data)
x = mpg
x = deparse(substitute(mpg))
x
substitute(mpg)
runApp('learn-to-shine/compare-selectors/compareSelectors.R')
is.list
?is.list
caption = list(text = "sometext", title = "some title")
caption
caption$text
l = function() { paste ("here") }
caption = list(text = l(), title = "some title")
caption
deparse(substitute(l()))
deparse(substitute(l)
)
deparse(substitute(l))
runApp('learn-to-shine/compare-selectors/compareSelectors.R')
runApp('learn-to-shine/compare-selectors/compareSelectors.R')
l = "l"
t = "t"
cap = list(t = t, l = l)
cap
digest::digest(get(cap), algo = sha256)
digest::digest(get(cap), algo = "sha256")
digest::digest(get(cap$t), algo = "sha256")
names(cap)
runApp('learn-to-shine/compare-selectors/compareSelectors.R')
runApp('learn-to-shine/compare-selectors/modules')
runApp('learn-to-shine/compare-selectors/compareSelectors.R')
shiny::runApp('shiny-selectors/modules')
switch(source$scenario_type(),
"ASlib" = textInput("scenario", label = h4(strong("Type ASlib scenario")),
placeholder = "ex. SAT11-INDU", value = "SAT11-INDU"),
"Custom" =  list(shinyDirButton("scenario_upload", label = "Upload scenario",
"Select directory with scenario"),
verbatimTextOutput("scenario_dir", placeholder = TRUE))
)
runApp('shiny-selectors/modules')
runApp('shiny-selectors/modules')
runApp('shiny-selectors/modules')
runApp('shiny-selectors/modules')
runApp('shiny-selectors/modules')
runApp('shiny-selectors/modules')
runApp('shiny-selectors/modules')
runApp()
runApp('shiny-selectors/modules')
runApp('shiny-selectors/modules')
runApp('shiny-selectors/modules')
runApp('shiny-selectors/modules')
runApp('shiny-selectors/modules')
runApp('shiny-selectors/modules')
runApp('shiny-selectors/modules')
runApp('shiny-selectors/modules')
runApp('shiny-selectors/modules')
runApp('shiny-selectors/modules')
runApp('shiny-selectors/modules')
runApp('shiny-selectors/modules')
runApp('shiny-selectors/modules')
runApp('shiny-selectors/modules')
runApp('shiny-selectors/modules')
sc = getCosealASScenario("SAT11-INDU")
summary(sc)
runApp('shiny-selectors/modules')
read_scenario(input$scenario_type, global$datapath, input$scenario)
runApp('shiny-selectors/modules')
runApp('shiny-selectors/modules-learner')
?shinyDirChoose
runApp('shiny-selectors/experiments/experimentLayout.R')
runApp('shiny-selectors/experiments/experimentLayout.R')
runApp('shiny-selectors/experiments/experimentLayout.R')
?fileInput
runApp('shiny-selectors/experiments/experimentLayout.R')
runApp('shiny-selectors/experiments/experimentLayout.R')
runApp('shiny-selectors/experiments/experimentLayout.R')
runApp('shiny-selectors/compareSelectors.R')
runApp('shiny-selectors/compareSelectors.R')
?load
ls()
var_name = "sc"
?deparse
v = deparse(var_name)
v
v = deparse(eval(var_name))
v
v =  deparse(substitute(var_name))
v
v = substitute(var_name)
v
v = deparse(var_name)
v
v = deparse("sc")
v
v = substitute("sc")
v
v = deparse(substitute("sc"))
v
var_name = load(sprintf("~/algorithm-selection-data/results-pair-standard/standard_%s.RData",  scenario$desc$scenario_id))
file_name = "~/algorithm-selection-data/results-pair-standard/standard_SAT11-HAND.RData"
file_name
var_name = load(file_name)
var_name
model = deparse(var_name)
model
model = deparse(substitvar_name)
model = deparse(substitute(var_name))
model
model = get(var_name)
model
sc
llama.cv = convertToLlamaCVFolds(sc)
data = fixFeckingPresolve(sc, llama.cv)
data$test
data$test[[1]]
head(data$data)
runApp('shiny-selectors/experiments/experimentLayout.R')
runApp('shiny-selectors/experiments/experimentLayout.R')
runApp('shiny-selectors/compareSelectors.R')
runApp('shiny-selectors/experiments/experimentLayout.R')
runApp('shiny-selectors/experiments/experimentLayout.R')
runApp('shiny-selectors/experiments/experimentLayout.R')
runApp('shiny-selectors/experiments/experimentLayout.R')
data$test
data$test[1:10]
data$test[[1:10]]
data$test[[1]]
data$test[[2]]
ids = rbind(data$test)
ids
ids = rbind(lapply(data$test, function(x) {x}))
ids
lapply(data$test, function(x) x)
lapply(data$test, function(x) unlist(x))
ids = unlist(data$test)
ids
runApp('shiny-selectors/experiments/experimentLayout.R')
library(shiny); runApp('shiny-selectors/experiments/experimentLayout.R')
global
library(shiny); runApp('shiny-selectors/experiments/experimentLayout.R')
source('shiny-selectors/experiments/experimentLayout.R')
runApp('shiny-selectors/experiments/experimentLayout.R')
library(shiny); runApp('shiny-selectors/compareSelectors.R')
runApp('shiny-selectors/compareSelectors.R')
runApp('shiny-selectors/compareSelectors.R')
runApp('shiny-selectors/compareSelectors.R')
runApp('shiny-selectors/compareSelectors.R')
runApp('shiny-selectors/compareSelectors.R')
runApp('shiny-selectors/compareSelectors.R')
runApp('shiny-selectors/compareSelectors.R')
runApp('shiny-selectors/compareSelectors.R')
runApp('shiny-selectors/compareSelectors.R')
runApp('shiny-selectors/compareSelectors.R')
runApp('shiny-selectors/compareSelectors.R')
runApp('shiny-selectors/compareSelectors.R')
runApp('shiny-selectors/compareSelectors.R')
library(shiny); runApp('shiny-selectors/compareSelectors.R')
runApp('shiny-selectors/experiments/experimentUI.R')
runApp('shiny-selectors/experiments/experimentUI.R')
runApp('experimentUI.R')
runApp('shiny-selectors/experiments/experimentUI.R')
runApp('shiny-selectors/experiments/experimentUI.R')
runApp('shiny-selectors/experiments/experimentUI.R')
runApp('shiny-selectors/experiments/experimentUI.R')
runApp('shiny-selectors/experiments/upload.R')
ui = pageWithSidebar(
headerPanel("Example"),
sidebarPanel(
textInput("path", "File:"),
actionButton("browse", "Browse"),
tags$br(),
actionButton("upload", "Upload Data")
),
mainPanel(
verbatimTextOutput('content')
)
)
runApp('shiny-selectors/experiments/upload.R')
runApp('shiny-selectors/experiments/experimentUI.R')
runApp('shiny-selectors/experiments/experimentUI.R')
runApp('shiny-selectors/experiments/experimentUI.R')
runApp('shiny-selectors/compareSelectors.R')
runApp('shiny-selectors/experiments/upload.R')
?observe
vignette(package = "shinyFiles")
vignette(shinyFiles)
vignette("shinyFiles")
?req
runApp('shiny-selectors/experiments/staticUI.R')
runApp('shiny-selectors/experiments/staticUI.R')
runApp('shiny-selectors/experiments/staticUI.R')
runApp('shiny-selectors/experiments/staticUI.R')
getwd()
setwd("./shiny-selectors/example-inputs/")
load("id.RData")
ls()
m1= combined_model
load("metrics.RData")
m2 = combined_model
head(m1$predictions)
head(m2$predictions)
sc = parseASScenario("~/modeling-algorithmic-performance/Models/aslib_scenarios/SAT11-INDU/")
sc = parseASScenario("../../modeling-algorithmic-performance/Models/aslib_scenarios/SAT11-INDU/")
llama.cv = convertToLlamaCVFolds(sc)
data = fixFeckingPresolve(sc, llama.cv)
p1 = misclassificationPenalties(data, m1)
p1 = misclassificationPenalties(data, m2)
head(p1)
runApp('~/shiny-selectors/experiments/staticUI.R')
runApp('~/shiny-selectors/experiments/staticUI.R')
runApp('~/shiny-selectors/experiments/staticUI.R')
runApp('~/shiny-selectors/experiments/staticUI.R')
runApp('~/shiny-selectors/experiments/staticUI.R')
print(model1)
print(m1)
runApp('~/shiny-selectors/experiments/upload-experiment.R')
runApp('~/shiny-selectors/experiments/upload-experiment.R')
runApp('~/shiny-selectors/experiments/upload-experiment.R')
runApp('~/shiny-selectors/experiments/upload-experiment.R')
runApp('~/shiny-selectors/experiments/upload-experiment.R')
sc
print(sc)
data$data[unlist(data)$test]
data$data[unlist(data$test)]
data$data[unlist(data$test), data$ids]
head(data$data[unlist(data$test), data$ids])
runApp('~/shiny-selectors/experiments/upload-experiment.R')
runApp('~/shiny-selectors/experiments/upload-experiment.R')
runApp('~/shiny-selectors/experiments/upload-experiment.R')
print(data)
print(data$data)
print(head(data))
print(head(data))
print(data)
runApp('~/shiny-selectors/experiments/upload-experiment.R')
runApp('~/shiny-selectors/experiments/upload-experiment.R')
data
print(Data)
print(data)
data
runApp('~/shiny-selectors/experiments/upload-experiment.R')
runApp('~/shiny-selectors/experiments/upload-experiment.R')
print(sc)
runApp('~/shiny-selectors/experiments/upload-experiment.R')
runApp('~/shiny-selectors/experiments/upload-experiment.R')
print(sc)
print(sc$desc)
print(sc$feature.runstatus)
runApp('~/shiny-selectors/experiments/upload-experiment.R')
runApp('~/shiny-selectors/experiments/upload-experiment.R')
runApp('~/shiny-selectors/experiments/staticUI.R')
c(1:30)
runApp('~/shiny-selectors/experiments/staticUI.R')
runApp('~/shiny-selectors/experiments/staticUI.R')
single = llama::singleBest(data)
single = llama::singleBest(llama.cv)
head(single)
single = llama:::singleBest(llama.cv)
runApp('~/shiny-selectors/experiments/staticUI.R')
runApp('~/shiny-selectors/experiments/staticUI.R')
scenario = getCosealASScenario(scenario_name)
runApp('~/shiny-selectors/experiments/staticUI.R')
runApp('~/shiny-selectors/experiments/staticUI.R')
sc
sc = getCosealASScenario("SAT11-RAND")
print("aslib")
runApp('~/shiny-selectors/experiments/staticUI.R')
runApp('~/shiny-selectors/experiments/staticUI.R')
runApp('~/shiny-selectors/compareSelectors.R')
runApp('~/shiny-selectors/experiments/upload-experiment.R')
runApp('~/shiny-selectors/experiments/upload-experiment.R')
runApp('~/shiny-selectors/experiments/upload-experiment.R')
runApp('~/shiny-selectors/experiments/upload-experiment.R')
print(sc)
print(sc$desc)
runApp('~/shiny-selectors/experiments/upload-experiment.R')
runApp('~/shiny-selectors/experiments/upload-experiment.R')
m1
m1$predictions
runApp('~/shiny-selectors/experiments/upload-experiment.R')
runApp('~/shiny-selectors/experiments/upload-experiment.R')
